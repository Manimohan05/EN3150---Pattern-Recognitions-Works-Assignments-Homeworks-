{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the penguin's dataset using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       species  class_encoded\n",
      "0       Adelie              0\n",
      "1       Adelie              0\n",
      "2       Adelie              0\n",
      "4       Adelie              0\n",
      "5       Adelie              0\n",
      "..         ...            ...\n",
      "215  Chinstrap              1\n",
      "216  Chinstrap              1\n",
      "217  Chinstrap              1\n",
      "218  Chinstrap              1\n",
      "219  Chinstrap              1\n",
      "\n",
      "[214 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "# Display the filtered and encoded DataFrame\n",
    "print(df_filtered[['species', 'class_encoded']])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "X = df_filtered.drop(['species', 'island', 'sex','class_encoded'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.What is the purpose of \"y_encoded = le.fit_transform(df_filtered['species'])\" ?\n",
    "\n",
    "The purpose of this line of code is to encode the target variable 'species' into numerical labels. In machine learning, algorithms often require the target variable to be in numerical form for training. The LabelEncoder from scikit-learn is used to convert the species names (e.g., 'Adelie' and 'Chinstrap') into corresponding numerical labels (e.g., 0 and 1), which can be used for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.What is the purpose of \"X = df.drop(['species', 'island', 'sex'], axis=1)\" ?\n",
    "\n",
    "X = df_filtered.drop(['species', 'island', 'sex'], axis=1) creates the feature matrix X by dropping the columns 'species', 'island', and 'sex' from the DataFrame. These columns are removed because they are categorical and not directly usable as features for logistic regression. You typically need to convert categorical variables into numerical representations, or in some cases, perform one-hot encoding, which you later do in the code.\n",
    "\n",
    "This line of code is used to create the feature matrix 'X' by dropping the columns 'species', 'island', and 'sex' from the original DataFrame 'df'. These columns are typically not used as features for classification in this specific context. The resulting 'X' contains only the numeric features that will be used to train the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Why we cannot use \"island\" and \"sex\" features?\n",
    "\n",
    "\"Island\" and \"sex\" are categorical features. While some machine learning algorithms can handle categorical data directly, logistic regression typically requires numeric input features. To use categorical features in logistic regression, they need to be one-hot encoded or otherwise transformed into numeric representations. In this code, these columns are dropped instead of being one-hot encoded, which is a common preprocessing step when dealing with categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the logistic regression model. Here we are using saga solver to learn weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[ 2.75633615e-03 -8.08986406e-05  4.77783153e-04 -2.87299611e-04]] [-8.39446233e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Why is accuracy low? why does the saga solver perform poorly?\n",
    "\n",
    "\n",
    "The accuracy of the logistic regression model with the 'saga' solver might be low because the 'saga' solver is sensitive to feature scaling. If features are not properly scaled, it can affect the convergence of the algorithm and lead to suboptimal results. This is why you observe an increase in accuracy when switching to the 'liblinear' solver, which is less sensitive to feature scaling.\n",
    "\n",
    "The initial accuracy might be low for several reasons:\n",
    "\n",
    "The features might not be well-suited for classification.\n",
    "\n",
    "There could be class imbalance in the dataset.\n",
    "\n",
    "The choice of solver ('saga') may not be optimal for this specific dataset.\n",
    "\n",
    "The 'saga' solver might perform poorly in this case because it's sensitive to the scale of the features, and logistic regression generally benefits from feature scaling. If the features are not properly scaled, it can affect the convergence and performance of the solver.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the solver to \"liblinear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[ 1.61343591 -1.4665703  -0.15152349 -0.00398479]] [-0.08866849]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Why is accuracy now? why does the \"liblinear\" solver perform better than \"saga\" solver ?\n",
    "\n",
    "Changing the solver to \"liblinear\" often improves accuracy, especially if the data is not well-scaled. \"liblinear\" is more robust to unscaled features, and it's a good choice when you have a small dataset or when other solvers do not perform well. Accuracy may increase because \"liblinear\" can handle the data better in its original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above tasks after feature normalization and observe the accuracy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Now observe the accuracies for both  \"liblinear\" solver and \"saga\" solver. Why accuracy of the \"saga\" solver is increased?\n",
    "\n",
    "Normalizing the features using StandardScaler scales them to have a mean of 0 and a standard deviation of 1. This can help the 'saga' solver converge faster and perform better because it reduces the impact of feature scales on the optimization process. Normalization often makes the solver more stable and effective, especially when features have different scales.\n",
    "\n",
    "\n",
    "Extra\n",
    "\n",
    "The accuracy of the \"saga\" solver may have increased after feature normalization (using the \"MaxAbsScaler\") because feature scaling can have a significant impact on the performance of logistic regression, especially when using the \"saga\" solver. Here's why the accuracy of the \"saga\" solver might have improved:\n",
    "\n",
    "Feature Scaling: The \"saga\" solver is sensitive to the scale of the features. When features have different scales, it can lead to slow convergence or convergence to suboptimal solutions. In the original, unscaled data, features like \"bill_length_mm\" and \"bill_depth_mm\" could have different scales. Scaling these features to have similar magnitudes can help the solver converge more quickly and reach a better solution.\n",
    "\n",
    "Normalization Effect: Normalization, in this case using \"MaxAbsScaler,\" can improve the condition of the optimization problem. It ensures that each feature has values within a similar range ([-1, 1]), making the optimization landscape more favorable. This can help the \"saga\" solver find a better decision boundary, leading to improved classification accuracy.\n",
    "\n",
    "Reducing Numerical Instabilities: Feature scaling can reduce numerical instabilities that might occur during the optimization process. When features are on different scales, it can cause problems like floating-point overflows or underflows, which can affect the solver's performance. Scaling mitigates these issues.\n",
    "\n",
    "Convergence: The \"saga\" solver is designed for large datasets and can handle both L1 and L2 regularization. In some cases, with scaled features, it may converge more efficiently and find a solution that separates the classes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to load the dataset again, and use logistic regression for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Dream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m logreg \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[39m#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m logreg\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Predict on the testing data\u001b[39;00m\n\u001b[0;32m     43\u001b[0m y_pred \u001b[39m=\u001b[39m logreg\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[39m=\u001b[39m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39msolver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Dream'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "\n",
    "df_filtered.head()\n",
    "\n",
    "X = df_filtered.drop(['species', 'class_encoded'], axis=1)  # Choose features\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for the  above error\n",
    "\n",
    "The error we encountered is due to the presence of non-numeric (string) values in your DataFrame columns. In particular, the issue is related to the 'island','sex' columns, which contais categorical string values. Logistic Regression, like many other machine learning models, requires numeric input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes made to address the error:\n",
    "\n",
    "Import OneHotEncoder from sklearn.preprocessing.\n",
    "\n",
    "Added 'sex' column to the list of columns to be one-hot encoded: pd.get_dummies(df_filtered, columns=['island', 'sex'], drop_first=True).\n",
    "\n",
    "Removed any references to the original 'sex' column after encoding since it's no longer needed for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.What is the problem? Why algorithm cannot perform classification?\n",
    "\n",
    "The problem is that the categorical features 'island' and 'sex' have not been transformed into numerical representations (e.g., one-hot encoding or label encoding). Logistic regression, as implemented here, requires numerical features. Without encoding these categorical features, the algorithm cannot process them, leading to errors or poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.How to solve this issue??\n",
    "\n",
    "To solve the issue in the second code block, you should transform the categorical features 'island' and 'sex' into numerical representations. One common approach is to use one-hot encoding, which creates binary columns for each category. You can use the following code to perform one-hot encoding:\n",
    "\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['island', 'sex'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[ 2.75545030e-03 -8.65124398e-05  4.49807168e-04 -2.85834541e-04\n",
      "   1.85223496e-04 -1.04988406e-04  1.07637223e-05]] [-8.6399497e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "# One-hot encode the 'island' and 'sex' columns\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['island', 'sex'], drop_first=True)\n",
    "\n",
    "df_filtered.head()\n",
    "\n",
    "X = df_filtered.drop(['species', 'class_encoded'], axis=1)  # Choose features\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to visualize the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0  Adelie            39.1           18.7              181.0       3750.0   \n",
      "1  Adelie            39.5           17.4              186.0       3800.0   \n",
      "\n",
      "   class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0              0             0                 1         1  \n",
      "1              0             0                 1         0  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "20  Adelie            37.8           18.3              174.0       3400.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0             0                 1         1  \n",
      "20              0             0                 0         0  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "30  Adelie            39.5           16.7              178.0       3250.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0             0                 1         1  \n",
      "30              0             1                 0         0  \n"
     ]
    }
   ],
   "source": [
    "samples = df_filtered.groupby('sex_Male').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Torgersen').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Dream').head(1)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to apply logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 7) (214,)\n",
      "Accuracy: 1.0\n",
      "[[ 3.63420412  0.16314238  0.62632368  0.10221005  2.59927011 -0.87718394\n",
      "  -0.35907275]] [-5.99637185]\n"
     ]
    }
   ],
   "source": [
    "X = df_filtered.drop(['species','class_encoded'], axis=1)\n",
    "\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "print(X.shape, y.shape)\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga',max_iter=150,)\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are using the \"MaxAbsScaler\" scaler rather than the \"StandardScaler\"?\n",
    "\n",
    "The choice of using \"MaxAbsScaler\" versus \"StandardScaler\" depends on the nature of the data and the goals of scaling. \"MaxAbsScaler\" scales the features by dividing each feature by its maximum absolute value. This scaler is useful when you want to preserve the sparsity of the data, as it doesn't shift the distribution of the data or affect the mean and variance.\n",
    "\n",
    "In contrast, \"StandardScaler\" standardizes the features to have a mean of 0 and a standard deviation of 1, which can be beneficial when features have different scales and you want to give them equal importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to visualize feature scaling before and after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59655172 0.98139535 0.93396226 0.91666667 0.         1.\n",
      "  1.        ]\n",
      " [0.8862069  0.88372093 0.94811321 0.82291667 1.         0.\n",
      "  1.        ]\n",
      " [0.68275862 0.8        0.9245283  0.73958333 0.         1.\n",
      "  0.        ]\n",
      " [0.87586207 0.86046512 0.94811321 0.92708333 1.         0.\n",
      "  1.        ]\n",
      " [0.7137931  0.86046512 0.95283019 0.80729167 0.         1.\n",
      "  1.        ]\n",
      " [0.64310345 0.95348837 0.93867925 0.78645833 0.         1.\n",
      "  1.        ]\n",
      " [0.65172414 0.85116279 0.82075472 0.70833333 0.         0.\n",
      "  0.        ]\n",
      " [0.5862069  0.79534884 0.87264151 0.70833333 1.         0.\n",
      "  0.        ]\n",
      " [0.73965517 0.81860465 0.9245283  0.97916667 0.         1.\n",
      "  1.        ]\n",
      " [0.62068966 0.79534884 0.88207547 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.82068966 0.85116279 0.91981132 0.80208333 1.         0.\n",
      "  0.        ]\n",
      " [0.80517241 0.83255814 0.91981132 0.6875     1.         0.\n",
      "  0.        ]\n",
      " [0.63103448 0.85581395 0.86792453 0.72395833 1.         0.\n",
      "  0.        ]\n",
      " [0.72586207 0.88837209 0.91981132 0.83333333 0.         1.\n",
      "  1.        ]\n",
      " [0.73275862 0.77674419 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.8362069  0.81395349 0.9009434  0.70833333 1.         0.\n",
      "  1.        ]\n",
      " [0.87758621 0.83255814 0.9245283  0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.84186047 0.87264151 0.82291667 0.         0.\n",
      "  1.        ]\n",
      " [0.69137931 0.87906977 0.88679245 0.89583333 0.         0.\n",
      "  1.        ]\n",
      " [0.80862069 0.77209302 0.90566038 0.5625     1.         0.\n",
      "  0.        ]\n",
      " [0.65344828 0.86511628 0.91037736 0.609375   0.         0.\n",
      "  0.        ]\n",
      " [0.96206897 0.92093023 0.97641509 0.83333333 1.         0.\n",
      "  1.        ]\n",
      " [0.85517241 0.84651163 0.91037736 0.78645833 1.         0.\n",
      "  1.        ]\n",
      " [0.85862069 0.80465116 0.93396226 0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.88448276 0.89302326 0.91037736 0.76041667 1.         0.\n",
      "  1.        ]\n",
      " [0.65689655 0.76744186 0.93396226 0.796875   0.         0.\n",
      "  0.        ]\n",
      " [0.67586207 0.98139535 0.9245283  0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.68103448 0.77674419 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.91034483 0.93023256 0.96698113 0.94791667 1.         0.\n",
      "  1.        ]\n",
      " [0.65       0.86976744 0.8490566  0.75       0.         0.\n",
      "  1.        ]\n",
      " [0.71206897 0.98139535 0.91981132 0.91666667 0.         0.\n",
      "  1.        ]\n",
      " [0.64310345 0.78139535 0.90566038 0.625      1.         0.\n",
      "  0.        ]\n",
      " [0.81034483 0.80465116 0.87264151 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.74482759 0.86046512 0.90566038 0.85416667 1.         0.\n",
      "  1.        ]\n",
      " [0.73793103 0.86046512 0.91981132 0.88541667 0.         1.\n",
      "  1.        ]\n",
      " [0.85       0.9255814  0.95754717 0.84375    1.         0.\n",
      "  1.        ]\n",
      " [0.89655172 0.88372093 0.92924528 0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.79482759 0.84651163 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.65172414 0.93023256 0.89622642 0.88541667 0.         0.\n",
      "  1.        ]\n",
      " [0.61551724 0.8372093  0.95283019 0.73958333 1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.93023256 0.89622642 0.8125     0.         0.\n",
      "  1.        ]\n",
      " [0.73275862 0.80465116 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.61206897 0.75348837 0.91981132 0.69791667 0.         0.\n",
      "  0.        ]]\n",
      "[[-1.34082659  2.35505035  0.88068888  1.62029553 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.80078227  0.5480021   1.30057122  0.57562238  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.40582395 -1.0008964   0.60076732 -0.35297599 -1.14490646  1.80969611\n",
      "  -0.98260737]\n",
      " [ 1.68858195  0.11775252  1.30057122  1.73637033  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.069223    0.11775252  1.440532    0.40151018 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.83592516  1.83875085  1.02064966  0.16936059 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.7424249  -0.05434732 -2.47836983 -0.70120037 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.4530269  -1.08694632 -0.93880125 -0.70120037  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.21127779 -0.65669673  0.60076732  2.3167443  -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-1.07902585 -1.08694632 -0.65887969 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.09018027 -0.05434732  0.46080654  0.34347279  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.92187979 -0.39854698  0.46080654 -0.93334996  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.96682553  0.0317026  -1.07876203 -0.52708818  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.06167737  0.63405202  0.46080654  0.69169717 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 0.13647758 -1.43114598 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.25848074 -0.74274665 -0.09903657 -0.70120037  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.70728201 -0.39854698  0.60076732 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469 -0.22644715 -0.93880125  0.57562238 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.31232369  0.46195218 -0.51891891  1.38814594 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.9592799  -1.5171959   0.0409242  -2.3262475   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.72372485  0.20380243  0.18088498 -1.80391093 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.62358459  1.23640143  2.1403359   0.69169717  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.46418132 -0.14039723  0.18088498  0.16936059  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.50158143 -0.91484648  0.88068888 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.78208222  0.72010193  0.18088498 -0.1208264   0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.68632474 -1.60324582  0.88068888  0.28543539 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.48062416  2.35505035  0.60076732  1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.424524   -1.43114598 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.06258301  1.40850127  1.86041434  1.96851992  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.76112495  0.28985235 -1.63860515 -0.23690119 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.08792305  2.35505035  0.46080654  1.62029553 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.83592516 -1.34509607  0.0409242  -1.62979873  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.97797995 -0.91484648 -0.93880125 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.26737795  0.11775252  0.0409242   0.92384676  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.19257774  0.11775252  0.46080654  1.27207115 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.40808116  1.32245135  1.58049278  0.80777197  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.91298259  0.5480021   0.7407281   1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.80967948 -0.14039723 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.7424249   1.40850127 -0.23899735  1.27207115 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-1.13512601 -0.31249707  1.440532   -0.35297599  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469  1.40850127 -0.23899735  0.45954758 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.13647758 -0.91484648 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.17252611 -1.86139557  0.46080654 -0.81727517 -1.14490646 -0.55257896\n",
      "  -0.98260737]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe in the values related to \"island_Dream\",    \"island_Torgersen\"  and   \"sex_Male\" features before and after scaling?\n",
    "\n",
    "Before scaling, the values related to these categorical features are either 0 or 1 because they are binary (indicating the presence or absence of a category). After scaling, using either \"MaxAbsScaler\" or \"StandardScaler\" doesn't change the values for these binary features since their maximum absolute value is 1. So, the values for \"island_Dream,\" \"island_Torgersen,\" and \"sex_Male\" remain the same (0 or 1) before and after scaling. Scaling primarily affects continuous numeric features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patrec",
   "language": "python",
   "name": "patrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
